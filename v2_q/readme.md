1. 统计文本长度的分布
   
   由于bert模型较大，需要限制上下文长度，因此统计所有新闻文本的长度（testa_length_statistics.py和train_length_statistics.py）发现几乎98%的长度都在4000以内，因此选4096作为文本的切割单位。

![length_distribution_detail.png](P:\25fall_nlp_contest\v2_q\asserts\length_distribution_detail.png)



2. 句号/感叹号等标点的统计推断

由于BERT是用于读懂文本完成完形填空任务的，因此句子结构的完整性和字符和位置信息很关键，而如果机械的按照4096的长度严格切分我们的新闻文本，假设一篇文本的长度是5000，将被切割成两份文档，4096+4，第二篇文档可能是一个截断的句子，这不好。

所以我们使用字符的统计信息(find_some_punctuation.py)推断句号和感叹号这种分隔符具体是哪个，统计文末出现概率前几名的字符（可能是句号，感叹号，问号等等），再结合这些符号的全文中出现的概率（全文中最高的应该依次是逗号/“的”、句号……），因此推断出句号是900，感叹号是2662.

![屏幕截图 2025-12-05 183303.png](P:\25fall_nlp_contest\v2_q\asserts\屏幕截图%202025-12-05%20183303.png)

![屏幕截图 2025-12-05 183322.png](P:\25fall_nlp_contest\v2_q\asserts\屏幕截图%202025-12-05%20183322.png)



3。生成与训练数据

按照前面的思路，按照4096 + 句号/感叹号智能切割文本得到用于预训练的文本(pretrain_data.py)

4.预训练

pre_train.py




