import os
import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizerFast
from safetensors.torch import load_file
from bertTextAttention import BertTextAttention

# ================= é…ç½® =================
# è·¯å¾„é…ç½®
BASE_DIR = "/data/jinda/qinmingge/BERT/25fallnewsclassify"
TEST_FILE = os.path.join(BASE_DIR, "test_a.csv")
SUBMIT_FILE = os.path.join(BASE_DIR, "submit_attention.csv")

# æ¨¡å‹è·¯å¾„
# 1. åŸå§‹é¢„è®­ç»ƒ BERT è·¯å¾„ (ç”¨äºåˆå§‹åŒ–æ¨¡å‹ç»“æ„)
PRETRAINED_BERT_PATH = os.path.join(BASE_DIR, "bert_small_4096_final")
# 2. å¾®è°ƒåçš„æƒé‡è·¯å¾„ (ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„å‚æ•°)
# ä¼˜å…ˆå°è¯• safetensorsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯• bin
FT_MODEL_PATH_SAFETENSORS = os.path.join(BASE_DIR, "output_ft_attention/final_model/model.safetensors")
FT_MODEL_PATH_BIN = os.path.join(BASE_DIR, "output_ft_attention/final_model/pytorch_model.bin")

# è¶…å‚æ•°
# ğŸš€ å¤šå¡å¹¶è¡Œé…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,3,5,7" # æŒ‡å®šç©ºé—²æ˜¾å¡ (GPU 2 æœ‰å ç”¨ï¼Œæ’é™¤)
# Attention æ¨¡å‹æ¯” CNN ç¨é‡ï¼ŒBatch Size è®¾ä¸º 32 * 4 = 128
BATCH_SIZE = 32 * 4 
SEQ_LEN = 4096
NUM_LABELS = 14
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= æ•°æ®é›†å®šä¹‰ =================
class TestDataset(Dataset):
    def __init__(self, csv_file):
        self.df = pd.read_csv(csv_file, sep='\t' if csv_file.endswith('.tsv') else ',')
        # æ£€æŸ¥åˆ—åï¼Œå¦‚æœæ˜¯ 'text' åˆ™ä½¿ç”¨ï¼Œå¦åˆ™å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ–‡æœ¬
        if 'text' in self.df.columns:
            self.texts = self.df['text'].tolist()
        else:
            print(f"âš ï¸ Warning: 'text' column not found in {csv_file}. Using first column.")
            self.texts = self.df.iloc[:, 0].tolist()
            
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {"text": str(self.texts[idx])} # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²

# ================= ä¸»å‡½æ•° =================
def main():
    print(f"ğŸš€ Starting Inference on {DEVICE}...")
    
    # 1. åŠ è½½ Tokenizer
    print(f"Loading tokenizer from {PRETRAINED_BERT_PATH}...")
    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_BERT_PATH)
    
    # 2. å‡†å¤‡æ•°æ®
    print(f"Loading test data from {TEST_FILE}...")
    test_dataset = TestDataset(TEST_FILE)
    print(f"Test set size: {len(test_dataset)}")
    
    # è‡ªå®šä¹‰ collate_fn å¤„ç† Tokenization
    def collate_fn(batch):
        texts = [item["text"] for item in batch]
        # åŠ¨æ€ Padding åˆ°å½“å‰ Batch æœ€é•¿
        encoding = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=SEQ_LEN, 
            return_tensors="pt"
        )
        return encoding

    test_loader = DataLoader(
        test_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=16, # ğŸš€ å¢åŠ  CPU è¿›ç¨‹æ•°ä»¥åŒ¹é… 5 å¼  GPU çš„åå
        pin_memory=True
    )
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    print(f"Initializing BertTextAttention model structure...")
    model = BertTextAttention(
        bert_model_path=PRETRAINED_BERT_PATH, 
        num_labels=NUM_LABELS
    )
    
    # 4. åŠ è½½å¾®è°ƒåçš„æƒé‡
    if os.path.exists(FT_MODEL_PATH_SAFETENSORS):
        print(f"Loading weights from {FT_MODEL_PATH_SAFETENSORS}...")
        state_dict = load_file(FT_MODEL_PATH_SAFETENSORS)
    elif os.path.exists(FT_MODEL_PATH_BIN):
        print(f"Loading weights from {FT_MODEL_PATH_BIN}...")
        state_dict = torch.load(FT_MODEL_PATH_BIN, map_location="cpu")
    else:
        raise FileNotFoundError(f"No model weights found at {FT_MODEL_PATH_SAFETENSORS} or {FT_MODEL_PATH_BIN}")

    # å¤„ç†å¯èƒ½çš„ key ä¸åŒ¹é…é—®é¢˜ (ä¾‹å¦‚ DDP è®­ç»ƒå¯èƒ½å¯¼è‡´ module. å‰ç¼€)
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith("module."):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
            
    model.load_state_dict(new_state_dict)
    
    # 5. å¤šå¡å¹¶è¡Œ (DataParallel)
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")
        model = torch.nn.DataParallel(model)
    
    model.to(DEVICE)
    model.eval()
    
    # 6. æ¨ç†å¾ªç¯
    all_preds = []
    
    print("Starting prediction loop...")
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Predicting"):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            
            # æ··åˆç²¾åº¦æ¨ç† (åŠ é€Ÿ)
            with torch.cuda.amp.autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs["logits"]
            
            # ä¿å­˜ logits
            all_preds.append(logits.cpu().numpy())
            
    # 7. ä¿å­˜ç»“æœ
    print("Concatenating logits...")
    all_logits = np.concatenate(all_preds, axis=0)
    
    print(f"Saving logits to {BASE_DIR}/submit_attention_logits.npy ...")
    np.save(os.path.join(BASE_DIR, "submit_attention_logits.npy"), all_logits)
    
    preds = np.argmax(all_logits, axis=1)
    print(f"Saving results to {SUBMIT_FILE}...")
    df_submit = pd.DataFrame({"label": preds})
    df_submit.to_csv(SUBMIT_FILE, index=False)
    
    print("âœ… Done!")

if __name__ == "__main__":
    main()
