2025-12-13 11:26:47 - INFO - Epoch: 0.05, Step: 50, Train Loss: 2.8008, Learning Rate: 9.66e-07
2025-12-13 11:28:52 - INFO - Epoch: 0.10, Step: 100, Train Loss: 2.4031, Learning Rate: 1.95e-06
2025-12-13 11:30:57 - INFO - Epoch: 0.15, Step: 150, Train Loss: 1.7845, Learning Rate: 2.94e-06
2025-12-13 11:33:02 - INFO - Epoch: 0.20, Step: 200, Train Loss: 1.1172, Learning Rate: 3.93e-06
2025-12-13 11:35:06 - INFO - Epoch: 0.25, Step: 250, Train Loss: 0.6480, Learning Rate: 4.91e-06
2025-12-13 11:37:11 - INFO - Epoch: 0.30, Step: 300, Train Loss: 0.4238, Learning Rate: 5.90e-06
2025-12-13 11:39:16 - INFO - Epoch: 0.35, Step: 350, Train Loss: 0.3418, Learning Rate: 6.88e-06
2025-12-13 11:41:21 - INFO - Epoch: 0.39, Step: 400, Train Loss: 0.2788, Learning Rate: 7.87e-06
2025-12-13 11:43:26 - INFO - Epoch: 0.44, Step: 450, Train Loss: 0.2283, Learning Rate: 8.86e-06
2025-12-13 11:45:31 - INFO - Epoch: 0.49, Step: 500, Train Loss: 0.2295, Learning Rate: 9.84e-06
2025-12-13 11:45:31 - INFO - Epoch: 0.49, Step: 500, Eval Loss: 0.2060, Accuracy: 0.9368, F1: 0.9198
2025-12-13 11:47:36 - INFO - Epoch: 0.54, Step: 550, Train Loss: 0.2186, Learning Rate: 1.08e-05
2025-12-13 11:49:41 - INFO - Epoch: 0.59, Step: 600, Train Loss: 0.2099, Learning Rate: 1.18e-05
2025-12-13 11:51:46 - INFO - Epoch: 0.64, Step: 650, Train Loss: 0.2000, Learning Rate: 1.28e-05
2025-12-13 11:53:51 - INFO - Epoch: 0.69, Step: 700, Train Loss: 0.1940, Learning Rate: 1.38e-05
2025-12-13 11:55:55 - INFO - Epoch: 0.74, Step: 750, Train Loss: 0.1739, Learning Rate: 1.48e-05
2025-12-13 11:58:00 - INFO - Epoch: 0.79, Step: 800, Train Loss: 0.1759, Learning Rate: 1.58e-05
2025-12-13 12:00:05 - INFO - Epoch: 0.84, Step: 850, Train Loss: 0.1637, Learning Rate: 1.67e-05
2025-12-13 12:02:10 - INFO - Epoch: 0.89, Step: 900, Train Loss: 0.1661, Learning Rate: 1.77e-05
2025-12-13 12:04:15 - INFO - Epoch: 0.94, Step: 950, Train Loss: 0.1598, Learning Rate: 1.87e-05
2025-12-13 12:06:20 - INFO - Epoch: 0.99, Step: 1000, Train Loss: 0.1572, Learning Rate: 1.97e-05
2025-12-13 12:06:20 - INFO - Epoch: 0.99, Step: 1000, Eval Loss: 0.1481, Accuracy: 0.9542, F1: 0.9473
2025-12-13 12:08:25 - INFO - Epoch: 1.04, Step: 1050, Train Loss: 0.1595, Learning Rate: 1.99e-05
2025-12-13 12:10:30 - INFO - Epoch: 1.08, Step: 1100, Train Loss: 0.1409, Learning Rate: 1.98e-05
2025-12-13 12:12:35 - INFO - Epoch: 1.13, Step: 1150, Train Loss: 0.1408, Learning Rate: 1.97e-05
2025-12-13 12:14:39 - INFO - Epoch: 1.18, Step: 1200, Train Loss: 0.1437, Learning Rate: 1.96e-05
2025-12-13 12:16:44 - INFO - Epoch: 1.23, Step: 1250, Train Loss: 0.1361, Learning Rate: 1.95e-05
2025-12-13 12:18:49 - INFO - Epoch: 1.28, Step: 1300, Train Loss: 0.1363, Learning Rate: 1.94e-05
2025-12-13 12:20:54 - INFO - Epoch: 1.33, Step: 1350, Train Loss: 0.1314, Learning Rate: 1.93e-05
2025-12-13 12:22:59 - INFO - Epoch: 1.38, Step: 1400, Train Loss: 0.1271, Learning Rate: 1.92e-05
2025-12-13 12:25:04 - INFO - Epoch: 1.43, Step: 1450, Train Loss: 0.1391, Learning Rate: 1.90e-05
2025-12-13 12:27:09 - INFO - Epoch: 1.48, Step: 1500, Train Loss: 0.1378, Learning Rate: 1.89e-05
2025-12-13 12:27:09 - INFO - Epoch: 1.48, Step: 1500, Eval Loss: 0.1361, Accuracy: 0.9576, F1: 0.9538
2025-12-13 12:29:14 - INFO - Epoch: 1.53, Step: 1550, Train Loss: 0.1452, Learning Rate: 1.88e-05
2025-12-13 12:31:19 - INFO - Epoch: 1.58, Step: 1600, Train Loss: 0.1247, Learning Rate: 1.87e-05
2025-12-13 12:33:24 - INFO - Epoch: 1.63, Step: 1650, Train Loss: 0.1252, Learning Rate: 1.86e-05
2025-12-13 12:35:28 - INFO - Epoch: 1.68, Step: 1700, Train Loss: 0.1316, Learning Rate: 1.85e-05
2025-12-13 12:37:33 - INFO - Epoch: 1.73, Step: 1750, Train Loss: 0.1125, Learning Rate: 1.84e-05
2025-12-13 12:39:38 - INFO - Epoch: 1.78, Step: 1800, Train Loss: 0.1235, Learning Rate: 1.83e-05
2025-12-13 12:41:43 - INFO - Epoch: 1.82, Step: 1850, Train Loss: 0.1239, Learning Rate: 1.82e-05
2025-12-13 12:43:48 - INFO - Epoch: 1.87, Step: 1900, Train Loss: 0.1257, Learning Rate: 1.81e-05
2025-12-13 12:45:53 - INFO - Epoch: 1.92, Step: 1950, Train Loss: 0.1205, Learning Rate: 1.80e-05
2025-12-13 12:47:58 - INFO - Epoch: 1.97, Step: 2000, Train Loss: 0.1196, Learning Rate: 1.78e-05
2025-12-13 12:47:58 - INFO - Epoch: 1.97, Step: 2000, Eval Loss: 0.1239, Accuracy: 0.9607, F1: 0.9549
2025-12-13 12:50:03 - INFO - Epoch: 2.02, Step: 2050, Train Loss: 0.1178, Learning Rate: 1.77e-05
2025-12-13 12:52:08 - INFO - Epoch: 2.07, Step: 2100, Train Loss: 0.0992, Learning Rate: 1.76e-05
2025-12-13 12:54:13 - INFO - Epoch: 2.12, Step: 2150, Train Loss: 0.0943, Learning Rate: 1.75e-05
2025-12-13 12:56:17 - INFO - Epoch: 2.17, Step: 2200, Train Loss: 0.0939, Learning Rate: 1.74e-05
2025-12-13 12:58:22 - INFO - Epoch: 2.22, Step: 2250, Train Loss: 0.0890, Learning Rate: 1.73e-05
2025-12-13 13:00:27 - INFO - Epoch: 2.27, Step: 2300, Train Loss: 0.0998, Learning Rate: 1.72e-05
2025-12-13 13:02:32 - INFO - Epoch: 2.32, Step: 2350, Train Loss: 0.0879, Learning Rate: 1.71e-05
2025-12-13 13:04:37 - INFO - Epoch: 2.37, Step: 2400, Train Loss: 0.0947, Learning Rate: 1.70e-05
2025-12-13 13:06:42 - INFO - Epoch: 2.42, Step: 2450, Train Loss: 0.0980, Learning Rate: 1.69e-05
2025-12-13 13:08:47 - INFO - Epoch: 2.47, Step: 2500, Train Loss: 0.0942, Learning Rate: 1.67e-05
2025-12-13 13:08:47 - INFO - Epoch: 2.47, Step: 2500, Eval Loss: 0.1152, Accuracy: 0.9647, F1: 0.9607
2025-12-13 13:10:52 - INFO - Epoch: 2.51, Step: 2550, Train Loss: 0.0906, Learning Rate: 1.66e-05
2025-12-13 13:12:57 - INFO - Epoch: 2.56, Step: 2600, Train Loss: 0.0919, Learning Rate: 1.65e-05
2025-12-13 13:15:02 - INFO - Epoch: 2.61, Step: 2650, Train Loss: 0.0919, Learning Rate: 1.64e-05
2025-12-13 13:17:06 - INFO - Epoch: 2.66, Step: 2700, Train Loss: 0.0953, Learning Rate: 1.63e-05
2025-12-13 13:19:11 - INFO - Epoch: 2.71, Step: 2750, Train Loss: 0.0937, Learning Rate: 1.62e-05
2025-12-13 13:21:16 - INFO - Epoch: 2.76, Step: 2800, Train Loss: 0.0924, Learning Rate: 1.61e-05
2025-12-13 13:23:21 - INFO - Epoch: 2.81, Step: 2850, Train Loss: 0.0907, Learning Rate: 1.60e-05
2025-12-13 13:25:26 - INFO - Epoch: 2.86, Step: 2900, Train Loss: 0.0896, Learning Rate: 1.59e-05
2025-12-13 13:27:31 - INFO - Epoch: 2.91, Step: 2950, Train Loss: 0.1011, Learning Rate: 1.58e-05
2025-12-13 13:29:36 - INFO - Epoch: 2.96, Step: 3000, Train Loss: 0.0890, Learning Rate: 1.56e-05
2025-12-13 13:29:36 - INFO - Epoch: 2.96, Step: 3000, Eval Loss: 0.1171, Accuracy: 0.9633, F1: 0.9600
2025-12-13 13:31:41 - INFO - Epoch: 3.01, Step: 3050, Train Loss: 0.0932, Learning Rate: 1.55e-05
2025-12-13 13:33:46 - INFO - Epoch: 3.06, Step: 3100, Train Loss: 0.0693, Learning Rate: 1.54e-05
2025-12-13 13:35:51 - INFO - Epoch: 3.11, Step: 3150, Train Loss: 0.0723, Learning Rate: 1.53e-05
2025-12-13 13:37:55 - INFO - Epoch: 3.16, Step: 3200, Train Loss: 0.0745, Learning Rate: 1.52e-05
2025-12-13 13:40:00 - INFO - Epoch: 3.21, Step: 3250, Train Loss: 0.0674, Learning Rate: 1.51e-05
2025-12-13 13:42:05 - INFO - Epoch: 3.25, Step: 3300, Train Loss: 0.0703, Learning Rate: 1.50e-05
2025-12-13 13:44:10 - INFO - Epoch: 3.30, Step: 3350, Train Loss: 0.0746, Learning Rate: 1.49e-05
2025-12-13 13:46:15 - INFO - Epoch: 3.35, Step: 3400, Train Loss: 0.0757, Learning Rate: 1.48e-05
2025-12-13 13:48:20 - INFO - Epoch: 3.40, Step: 3450, Train Loss: 0.0702, Learning Rate: 1.47e-05
2025-12-13 13:50:25 - INFO - Epoch: 3.45, Step: 3500, Train Loss: 0.0711, Learning Rate: 1.46e-05
2025-12-13 13:50:25 - INFO - Epoch: 3.45, Step: 3500, Eval Loss: 0.1160, Accuracy: 0.9655, F1: 0.9621
2025-12-13 13:52:30 - INFO - Epoch: 3.50, Step: 3550, Train Loss: 0.0687, Learning Rate: 1.44e-05
2025-12-13 13:54:35 - INFO - Epoch: 3.55, Step: 3600, Train Loss: 0.0662, Learning Rate: 1.43e-05
2025-12-13 13:56:40 - INFO - Epoch: 3.60, Step: 3650, Train Loss: 0.0720, Learning Rate: 1.42e-05
2025-12-13 13:58:44 - INFO - Epoch: 3.65, Step: 3700, Train Loss: 0.0663, Learning Rate: 1.41e-05
2025-12-13 14:00:49 - INFO - Epoch: 3.70, Step: 3750, Train Loss: 0.0710, Learning Rate: 1.40e-05
2025-12-13 14:02:54 - INFO - Epoch: 3.75, Step: 3800, Train Loss: 0.0798, Learning Rate: 1.39e-05
2025-12-13 14:04:59 - INFO - Epoch: 3.80, Step: 3850, Train Loss: 0.0723, Learning Rate: 1.38e-05
2025-12-13 14:07:04 - INFO - Epoch: 3.85, Step: 3900, Train Loss: 0.0708, Learning Rate: 1.37e-05
2025-12-13 14:09:09 - INFO - Epoch: 3.90, Step: 3950, Train Loss: 0.0638, Learning Rate: 1.36e-05
2025-12-13 14:11:14 - INFO - Epoch: 3.94, Step: 4000, Train Loss: 0.0655, Learning Rate: 1.35e-05
2025-12-13 14:11:14 - INFO - Epoch: 3.94, Step: 4000, Eval Loss: 0.1170, Accuracy: 0.9650, F1: 0.9610
2025-12-13 14:13:19 - INFO - Epoch: 3.99, Step: 4050, Train Loss: 0.0754, Learning Rate: 1.33e-05
2025-12-13 14:15:24 - INFO - Epoch: 4.04, Step: 4100, Train Loss: 0.0495, Learning Rate: 1.32e-05
2025-12-13 14:17:28 - INFO - Epoch: 4.09, Step: 4150, Train Loss: 0.0563, Learning Rate: 1.31e-05
2025-12-13 14:19:33 - INFO - Epoch: 4.14, Step: 4200, Train Loss: 0.0606, Learning Rate: 1.30e-05
2025-12-13 14:21:38 - INFO - Epoch: 4.19, Step: 4250, Train Loss: 0.0596, Learning Rate: 1.29e-05
2025-12-13 14:23:43 - INFO - Epoch: 4.24, Step: 4300, Train Loss: 0.0496, Learning Rate: 1.28e-05
2025-12-13 14:25:48 - INFO - Epoch: 4.29, Step: 4350, Train Loss: 0.0576, Learning Rate: 1.27e-05
2025-12-13 14:27:53 - INFO - Epoch: 4.34, Step: 4400, Train Loss: 0.0514, Learning Rate: 1.26e-05
2025-12-13 14:29:58 - INFO - Epoch: 4.39, Step: 4450, Train Loss: 0.0586, Learning Rate: 1.25e-05
2025-12-13 14:32:03 - INFO - Epoch: 4.44, Step: 4500, Train Loss: 0.0571, Learning Rate: 1.24e-05
2025-12-13 14:32:03 - INFO - Epoch: 4.44, Step: 4500, Eval Loss: 0.1179, Accuracy: 0.9661, F1: 0.9639
2025-12-13 14:34:08 - INFO - Epoch: 4.49, Step: 4550, Train Loss: 0.0580, Learning Rate: 1.23e-05
2025-12-13 14:36:13 - INFO - Epoch: 4.54, Step: 4600, Train Loss: 0.0535, Learning Rate: 1.21e-05
2025-12-13 14:38:17 - INFO - Epoch: 4.59, Step: 4650, Train Loss: 0.0472, Learning Rate: 1.20e-05
2025-12-13 14:40:22 - INFO - Epoch: 4.64, Step: 4700, Train Loss: 0.0585, Learning Rate: 1.19e-05
2025-12-13 14:42:27 - INFO - Epoch: 4.68, Step: 4750, Train Loss: 0.0530, Learning Rate: 1.18e-05
2025-12-13 14:44:32 - INFO - Epoch: 4.73, Step: 4800, Train Loss: 0.0568, Learning Rate: 1.17e-05
2025-12-13 14:46:37 - INFO - Epoch: 4.78, Step: 4850, Train Loss: 0.0554, Learning Rate: 1.16e-05
2025-12-13 14:48:42 - INFO - Epoch: 4.83, Step: 4900, Train Loss: 0.0566, Learning Rate: 1.15e-05
2025-12-13 14:50:47 - INFO - Epoch: 4.88, Step: 4950, Train Loss: 0.0516, Learning Rate: 1.14e-05
2025-12-13 14:52:52 - INFO - Epoch: 4.93, Step: 5000, Train Loss: 0.0542, Learning Rate: 1.13e-05
2025-12-13 14:52:52 - INFO - Epoch: 4.93, Step: 5000, Eval Loss: 0.1253, Accuracy: 0.9640, F1: 0.9601
2025-12-13 14:54:57 - INFO - Epoch: 4.98, Step: 5050, Train Loss: 0.0519, Learning Rate: 1.12e-05
2025-12-13 14:57:02 - INFO - Epoch: 5.03, Step: 5100, Train Loss: 0.0508, Learning Rate: 1.10e-05
2025-12-13 14:59:06 - INFO - Epoch: 5.08, Step: 5150, Train Loss: 0.0392, Learning Rate: 1.09e-05
2025-12-13 15:01:11 - INFO - Epoch: 5.13, Step: 5200, Train Loss: 0.0375, Learning Rate: 1.08e-05
2025-12-13 15:03:16 - INFO - Epoch: 5.18, Step: 5250, Train Loss: 0.0404, Learning Rate: 1.07e-05
2025-12-13 15:05:21 - INFO - Epoch: 5.23, Step: 5300, Train Loss: 0.0420, Learning Rate: 1.06e-05
2025-12-13 15:07:26 - INFO - Epoch: 5.28, Step: 5350, Train Loss: 0.0499, Learning Rate: 1.05e-05
2025-12-13 15:09:31 - INFO - Epoch: 5.33, Step: 5400, Train Loss: 0.0428, Learning Rate: 1.04e-05
2025-12-13 15:11:36 - INFO - Epoch: 5.37, Step: 5450, Train Loss: 0.0422, Learning Rate: 1.03e-05
2025-12-13 15:13:41 - INFO - Epoch: 5.42, Step: 5500, Train Loss: 0.0377, Learning Rate: 1.02e-05
2025-12-13 15:13:41 - INFO - Epoch: 5.42, Step: 5500, Eval Loss: 0.1236, Accuracy: 0.9665, F1: 0.9629
2025-12-13 15:15:46 - INFO - Epoch: 5.47, Step: 5550, Train Loss: 0.0377, Learning Rate: 1.01e-05
2025-12-13 15:17:51 - INFO - Epoch: 5.52, Step: 5600, Train Loss: 0.0423, Learning Rate: 9.95e-06
2025-12-13 15:19:55 - INFO - Epoch: 5.57, Step: 5650, Train Loss: 0.0482, Learning Rate: 9.84e-06
2025-12-13 15:22:00 - INFO - Epoch: 5.62, Step: 5700, Train Loss: 0.0376, Learning Rate: 9.73e-06
2025-12-13 15:24:05 - INFO - Epoch: 5.67, Step: 5750, Train Loss: 0.0410, Learning Rate: 9.62e-06
2025-12-13 15:26:10 - INFO - Epoch: 5.72, Step: 5800, Train Loss: 0.0412, Learning Rate: 9.51e-06
2025-12-13 15:28:15 - INFO - Epoch: 5.77, Step: 5850, Train Loss: 0.0411, Learning Rate: 9.40e-06
2025-12-13 15:30:20 - INFO - Epoch: 5.82, Step: 5900, Train Loss: 0.0397, Learning Rate: 9.29e-06
2025-12-13 15:32:25 - INFO - Epoch: 5.87, Step: 5950, Train Loss: 0.0406, Learning Rate: 9.18e-06
2025-12-13 15:34:30 - INFO - Epoch: 5.92, Step: 6000, Train Loss: 0.0443, Learning Rate: 9.08e-06
2025-12-13 15:34:30 - INFO - Epoch: 5.92, Step: 6000, Eval Loss: 0.1270, Accuracy: 0.9666, F1: 0.9640
2025-12-13 15:36:35 - INFO - Epoch: 5.97, Step: 6050, Train Loss: 0.0398, Learning Rate: 8.97e-06
2025-12-13 15:38:40 - INFO - Epoch: 6.02, Step: 6100, Train Loss: 0.0427, Learning Rate: 8.86e-06
2025-12-13 15:40:44 - INFO - Epoch: 6.07, Step: 6150, Train Loss: 0.0263, Learning Rate: 8.75e-06
2025-12-13 15:42:49 - INFO - Epoch: 6.11, Step: 6200, Train Loss: 0.0311, Learning Rate: 8.64e-06
2025-12-13 15:44:54 - INFO - Epoch: 6.16, Step: 6250, Train Loss: 0.0298, Learning Rate: 8.53e-06
2025-12-13 15:46:59 - INFO - Epoch: 6.21, Step: 6300, Train Loss: 0.0277, Learning Rate: 8.42e-06
2025-12-13 15:49:04 - INFO - Epoch: 6.26, Step: 6350, Train Loss: 0.0317, Learning Rate: 8.31e-06
2025-12-13 15:51:09 - INFO - Epoch: 6.31, Step: 6400, Train Loss: 0.0286, Learning Rate: 8.20e-06
2025-12-13 15:53:14 - INFO - Epoch: 6.36, Step: 6450, Train Loss: 0.0341, Learning Rate: 8.09e-06
2025-12-13 15:55:19 - INFO - Epoch: 6.41, Step: 6500, Train Loss: 0.0339, Learning Rate: 7.98e-06
2025-12-13 15:55:19 - INFO - Epoch: 6.41, Step: 6500, Eval Loss: 0.1293, Accuracy: 0.9664, F1: 0.9633
2025-12-13 15:57:24 - INFO - Epoch: 6.46, Step: 6550, Train Loss: 0.0332, Learning Rate: 7.87e-06
2025-12-13 15:59:29 - INFO - Epoch: 6.51, Step: 6600, Train Loss: 0.0302, Learning Rate: 7.76e-06
2025-12-13 16:01:33 - INFO - Epoch: 6.56, Step: 6650, Train Loss: 0.0308, Learning Rate: 7.65e-06
2025-12-13 16:03:38 - INFO - Epoch: 6.61, Step: 6700, Train Loss: 0.0299, Learning Rate: 7.54e-06
2025-12-13 16:05:43 - INFO - Epoch: 6.66, Step: 6750, Train Loss: 0.0361, Learning Rate: 7.43e-06
2025-12-13 16:07:48 - INFO - Epoch: 6.71, Step: 6800, Train Loss: 0.0324, Learning Rate: 7.32e-06
2025-12-13 16:09:53 - INFO - Epoch: 6.76, Step: 6850, Train Loss: 0.0313, Learning Rate: 7.21e-06
2025-12-13 16:11:58 - INFO - Epoch: 6.80, Step: 6900, Train Loss: 0.0240, Learning Rate: 7.10e-06
2025-12-13 16:14:03 - INFO - Epoch: 6.85, Step: 6950, Train Loss: 0.0299, Learning Rate: 6.99e-06
2025-12-13 16:16:08 - INFO - Epoch: 6.90, Step: 7000, Train Loss: 0.0322, Learning Rate: 6.88e-06
2025-12-13 16:16:08 - INFO - Epoch: 6.90, Step: 7000, Eval Loss: 0.1314, Accuracy: 0.9666, F1: 0.9642
2025-12-13 16:18:13 - INFO - Epoch: 6.95, Step: 7050, Train Loss: 0.0351, Learning Rate: 6.77e-06
2025-12-13 16:20:17 - INFO - Epoch: 7.00, Step: 7100, Train Loss: 0.0368, Learning Rate: 6.66e-06
2025-12-13 16:22:22 - INFO - Epoch: 7.05, Step: 7150, Train Loss: 0.0227, Learning Rate: 6.55e-06
2025-12-13 16:24:27 - INFO - Epoch: 7.10, Step: 7200, Train Loss: 0.0265, Learning Rate: 6.45e-06
2025-12-13 16:26:32 - INFO - Epoch: 7.15, Step: 7250, Train Loss: 0.0231, Learning Rate: 6.34e-06
2025-12-13 16:28:37 - INFO - Epoch: 7.20, Step: 7300, Train Loss: 0.0259, Learning Rate: 6.23e-06
2025-12-13 16:30:42 - INFO - Epoch: 7.25, Step: 7350, Train Loss: 0.0211, Learning Rate: 6.12e-06
2025-12-13 16:32:47 - INFO - Epoch: 7.30, Step: 7400, Train Loss: 0.0252, Learning Rate: 6.01e-06
2025-12-13 16:34:52 - INFO - Epoch: 7.35, Step: 7450, Train Loss: 0.0242, Learning Rate: 5.90e-06
2025-12-13 16:36:57 - INFO - Epoch: 7.40, Step: 7500, Train Loss: 0.0272, Learning Rate: 5.79e-06
2025-12-13 16:36:57 - INFO - Epoch: 7.40, Step: 7500, Eval Loss: 0.1325, Accuracy: 0.9671, F1: 0.9638
2025-12-13 16:39:02 - INFO - Epoch: 7.45, Step: 7550, Train Loss: 0.0252, Learning Rate: 5.68e-06
2025-12-13 16:41:06 - INFO - Epoch: 7.50, Step: 7600, Train Loss: 0.0227, Learning Rate: 5.57e-06
2025-12-13 16:43:11 - INFO - Epoch: 7.54, Step: 7650, Train Loss: 0.0240, Learning Rate: 5.46e-06
2025-12-13 16:45:16 - INFO - Epoch: 7.59, Step: 7700, Train Loss: 0.0226, Learning Rate: 5.35e-06
2025-12-13 16:47:21 - INFO - Epoch: 7.64, Step: 7750, Train Loss: 0.0230, Learning Rate: 5.24e-06
2025-12-13 16:49:26 - INFO - Epoch: 7.69, Step: 7800, Train Loss: 0.0243, Learning Rate: 5.13e-06
2025-12-13 16:51:31 - INFO - Epoch: 7.74, Step: 7850, Train Loss: 0.0257, Learning Rate: 5.02e-06
2025-12-13 16:53:36 - INFO - Epoch: 7.79, Step: 7900, Train Loss: 0.0195, Learning Rate: 4.91e-06
2025-12-13 16:55:41 - INFO - Epoch: 7.84, Step: 7950, Train Loss: 0.0226, Learning Rate: 4.80e-06
2025-12-13 16:57:46 - INFO - Epoch: 7.89, Step: 8000, Train Loss: 0.0279, Learning Rate: 4.69e-06
2025-12-13 16:57:46 - INFO - Epoch: 7.89, Step: 8000, Eval Loss: 0.1367, Accuracy: 0.9666, F1: 0.9642
2025-12-13 16:59:51 - INFO - Epoch: 7.94, Step: 8050, Train Loss: 0.0252, Learning Rate: 4.58e-06
2025-12-13 17:01:55 - INFO - Epoch: 7.99, Step: 8100, Train Loss: 0.0247, Learning Rate: 4.47e-06
2025-12-13 17:04:00 - INFO - Epoch: 8.04, Step: 8150, Train Loss: 0.0206, Learning Rate: 4.36e-06
2025-12-13 17:06:05 - INFO - Epoch: 8.09, Step: 8200, Train Loss: 0.0176, Learning Rate: 4.25e-06
2025-12-13 17:08:10 - INFO - Epoch: 8.14, Step: 8250, Train Loss: 0.0174, Learning Rate: 4.14e-06
2025-12-13 17:10:15 - INFO - Epoch: 8.19, Step: 8300, Train Loss: 0.0156, Learning Rate: 4.03e-06
2025-12-13 17:12:20 - INFO - Epoch: 8.23, Step: 8350, Train Loss: 0.0181, Learning Rate: 3.93e-06
2025-12-13 17:14:25 - INFO - Epoch: 8.28, Step: 8400, Train Loss: 0.0207, Learning Rate: 3.82e-06
2025-12-13 17:16:30 - INFO - Epoch: 8.33, Step: 8450, Train Loss: 0.0170, Learning Rate: 3.71e-06
2025-12-13 17:18:35 - INFO - Epoch: 8.38, Step: 8500, Train Loss: 0.0184, Learning Rate: 3.60e-06
2025-12-13 17:18:35 - INFO - Epoch: 8.38, Step: 8500, Eval Loss: 0.1418, Accuracy: 0.9663, F1: 0.9638
2025-12-13 17:20:40 - INFO - Epoch: 8.43, Step: 8550, Train Loss: 0.0155, Learning Rate: 3.49e-06
2025-12-13 17:22:44 - INFO - Epoch: 8.48, Step: 8600, Train Loss: 0.0201, Learning Rate: 3.38e-06
2025-12-13 17:24:49 - INFO - Epoch: 8.53, Step: 8650, Train Loss: 0.0160, Learning Rate: 3.27e-06
2025-12-13 17:26:54 - INFO - Epoch: 8.58, Step: 8700, Train Loss: 0.0197, Learning Rate: 3.16e-06
2025-12-13 17:28:59 - INFO - Epoch: 8.63, Step: 8750, Train Loss: 0.0184, Learning Rate: 3.05e-06
2025-12-13 17:31:04 - INFO - Epoch: 8.68, Step: 8800, Train Loss: 0.0230, Learning Rate: 2.94e-06
2025-12-13 17:33:09 - INFO - Epoch: 8.73, Step: 8850, Train Loss: 0.0165, Learning Rate: 2.83e-06
2025-12-13 17:35:14 - INFO - Epoch: 8.78, Step: 8900, Train Loss: 0.0179, Learning Rate: 2.72e-06
2025-12-13 17:37:19 - INFO - Epoch: 8.83, Step: 8950, Train Loss: 0.0187, Learning Rate: 2.61e-06
2025-12-13 17:39:24 - INFO - Epoch: 8.88, Step: 9000, Train Loss: 0.0180, Learning Rate: 2.50e-06
2025-12-13 17:39:24 - INFO - Epoch: 8.88, Step: 9000, Eval Loss: 0.1430, Accuracy: 0.9667, F1: 0.9642
2025-12-13 17:41:29 - INFO - Epoch: 8.93, Step: 9050, Train Loss: 0.0199, Learning Rate: 2.39e-06
2025-12-13 17:43:33 - INFO - Epoch: 8.97, Step: 9100, Train Loss: 0.0216, Learning Rate: 2.28e-06
2025-12-13 17:45:38 - INFO - Epoch: 9.02, Step: 9150, Train Loss: 0.0199, Learning Rate: 2.17e-06
2025-12-13 17:47:43 - INFO - Epoch: 9.07, Step: 9200, Train Loss: 0.0151, Learning Rate: 2.06e-06
2025-12-13 17:49:48 - INFO - Epoch: 9.12, Step: 9250, Train Loss: 0.0181, Learning Rate: 1.95e-06
2025-12-13 17:51:53 - INFO - Epoch: 9.17, Step: 9300, Train Loss: 0.0115, Learning Rate: 1.84e-06
2025-12-13 17:53:58 - INFO - Epoch: 9.22, Step: 9350, Train Loss: 0.0163, Learning Rate: 1.73e-06
2025-12-13 17:56:03 - INFO - Epoch: 9.27, Step: 9400, Train Loss: 0.0156, Learning Rate: 1.62e-06
2025-12-13 17:58:08 - INFO - Epoch: 9.32, Step: 9450, Train Loss: 0.0171, Learning Rate: 1.51e-06
2025-12-13 18:00:13 - INFO - Epoch: 9.37, Step: 9500, Train Loss: 0.0134, Learning Rate: 1.40e-06
2025-12-13 18:00:13 - INFO - Epoch: 9.37, Step: 9500, Eval Loss: 0.1453, Accuracy: 0.9669, F1: 0.9646
2025-12-13 18:02:18 - INFO - Epoch: 9.42, Step: 9550, Train Loss: 0.0155, Learning Rate: 1.30e-06
2025-12-13 18:04:22 - INFO - Epoch: 9.47, Step: 9600, Train Loss: 0.0136, Learning Rate: 1.19e-06
2025-12-13 18:06:27 - INFO - Epoch: 9.52, Step: 9650, Train Loss: 0.0158, Learning Rate: 1.08e-06
2025-12-13 18:08:32 - INFO - Epoch: 9.57, Step: 9700, Train Loss: 0.0136, Learning Rate: 9.66e-07
2025-12-13 18:10:37 - INFO - Epoch: 9.62, Step: 9750, Train Loss: 0.0151, Learning Rate: 8.57e-07
2025-12-13 18:12:42 - INFO - Epoch: 9.66, Step: 9800, Train Loss: 0.0157, Learning Rate: 7.47e-07
2025-12-13 18:14:47 - INFO - Epoch: 9.71, Step: 9850, Train Loss: 0.0142, Learning Rate: 6.38e-07
2025-12-13 18:16:52 - INFO - Epoch: 9.76, Step: 9900, Train Loss: 0.0166, Learning Rate: 5.28e-07
2025-12-13 18:18:57 - INFO - Epoch: 9.81, Step: 9950, Train Loss: 0.0201, Learning Rate: 4.19e-07
2025-12-13 18:21:02 - INFO - Epoch: 9.86, Step: 10000, Train Loss: 0.0140, Learning Rate: 3.09e-07
2025-12-13 18:21:02 - INFO - Epoch: 9.86, Step: 10000, Eval Loss: 0.1438, Accuracy: 0.9669, F1: 0.9644
