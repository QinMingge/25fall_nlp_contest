import os
import sys
import logging
import torch
import numpy as np
from datasets import load_from_disk
from transformers import (
    Trainer, 
    TrainingArguments, 
    BertTokenizerFast,
    EarlyStoppingCallback,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score
from bertLINEAR import BertLinear
import datetime

import time

# ========================== ğŸ“‹ è‡ªå®šä¹‰æ—¥å¿—å›è°ƒ ==========================
class CustomLogCallback(TrainerCallback):
    """
    è‡ªå®šä¹‰æ—¥å¿—å›è°ƒï¼Œç”¨äºè¾“å‡ºç¬¦åˆè¦æ±‚çš„æ ¼å¼ï¼š
    Time - INFO - Epoch: X, Step: Y, Train Loss: Z, LR: ..., Speed: ...
    """
    def __init__(self):
        self.last_time = time.time()
        self.last_step = 0

    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            # è®¡ç®—é€Ÿåº¦
            current_time = time.time()
            time_delta = current_time - self.last_time
            step_delta = state.global_step - self.last_step
            
            # é¿å…é™¤ä»¥é›¶
            if step_delta > 0 and time_delta > 0:
                steps_per_sec = step_delta / time_delta
                ms_per_step = (time_delta / step_delta) * 1000
                # ä¼°ç®— samples per second (batch_size * steps_per_sec * num_gpus)
                # æ³¨æ„ï¼šargs.per_device_train_batch_size æ˜¯å•å¡ batch size
                # world_size å¯ä»¥é€šè¿‡ args.world_size è·å– (å¦‚æœ Trainer æ³¨å…¥äº†) æˆ–è€…æ‰‹åŠ¨è®¡ç®—
                # è¿™é‡Œç®€å•æ‰“å° ms/step
                speed_info = f"{ms_per_step:.2f}ms/step"
            else:
                speed_info = "N/A"
            
            # æ›´æ–°çŠ¶æ€
            self.last_time = current_time
            self.last_step = state.global_step

            # è·å–å½“å‰æ—¶é—´
            now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # æ„å»ºæ—¥å¿—ä¿¡æ¯ parts
            msg_parts = []
            
            # Epoch
            if 'epoch' in logs:
                msg_parts.append(f"Epoch: {logs['epoch']:.2f}")
            
            # Step
            msg_parts.append(f"Step: {state.global_step}")
            
            # Train Loss
            if 'loss' in logs:
                msg_parts.append(f"Train Loss: {logs['loss']:.4f}")
            
            # Learning Rate
            if 'learning_rate' in logs:
                msg_parts.append(f"LR: {logs['learning_rate']:.2e}")

            # Batch Size (å•å¡)
            msg_parts.append(f"Batch: {args.per_device_train_batch_size}")

            # Speed
            msg_parts.append(f"Speed: {speed_info}")

            # Eval Metrics (å¦‚æœæœ‰)
            if 'eval_loss' in logs:
                msg_parts.append(f"Eval Loss: {logs['eval_loss']:.4f}")
            if 'eval_accuracy' in logs:
                msg_parts.append(f"Accuracy: {logs['eval_accuracy']:.4f}")
            if 'eval_f1_macro' in logs:
                msg_parts.append(f"F1: {logs['eval_f1_macro']:.4f}")
                
            # ç»„åˆæ¶ˆæ¯
            log_msg = ", ".join(msg_parts)
            
            # è·å– logger
            logger = logging.getLogger(__name__)
            logger.info(log_msg)


# ========================== ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒº ==========================
# 1. è°ƒè¯•æ¨¡å¼å¼€å…³
DEBUG_MODE = False 

# 2. æ˜¾å¡æŒ‡å®š
if DEBUG_MODE:
    os.environ["CUDA_VISIBLE_DEVICES"] = "1"
else:
    # âš ï¸ 0,1,2,3,5 å·å¡ç©ºé—²
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,5" 

# 3. è·¯å¾„é…ç½®
DATA_DIR = "ft_data_stratified" # ğŸ†• ä½¿ç”¨åˆ†å±‚æŠ½æ ·æ•°æ®
PRETRAINED_MODEL_PATH = "bert_small_4096_final"

if DEBUG_MODE:
    OUTPUT_DIR = "output_ft_linear_debug"
else:
    OUTPUT_DIR = "output_ft_linear"

# 4. è®­ç»ƒè¶…å‚
SEQ_LEN = 4096
NUM_LABELS = 14

if DEBUG_MODE:
    NUM_EPOCHS = 1
    BATCH_SIZE = 4
    REPORT_TO = "none"
    SAVE_STRATEGY = "steps"
    EVAL_STRATEGY = "steps"
    EVAL_STEPS = 10
    SAVE_STEPS = 10
else:
    NUM_EPOCHS = 10
    # ğŸš€ æ¿€è¿›ä¼˜åŒ–ï¼šBatch Size 32
    # Linear æ¨¡å‹å‚æ•°å°‘ï¼Œæ˜¾å­˜å ç”¨ä½ï¼Œ32 åº”è¯¥å¾ˆå®‰å…¨
    BATCH_SIZE = 32 
    REPORT_TO = "tensorboard" # ğŸ†• å¯ç”¨ TensorBoard å¯è§†åŒ–
    
    SAVE_STRATEGY = "steps"
    EVAL_STRATEGY = "steps"
    EVAL_STEPS = 500
    SAVE_STEPS = 500

# ====================================================================

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='macro')
    
    return {
        'accuracy': acc,
        'f1_macro': f1
    }

def main():
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    is_main_process = local_rank in [-1, 0]

    # 0. è®¾ç½®æ—¥å¿—
    if is_main_process:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        log_file = os.path.join(OUTPUT_DIR, "training_detailed.log")
        
        logging.basicConfig(
            format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
            datefmt="%m/%d/%Y %H:%M:%S",
            level=logging.INFO,
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(log_file, mode="w")
            ]
        )
        logger = logging.getLogger(__name__)
        logger.info(f"Logging to {log_file}")
    else:
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.ERROR)

    # 1. åŠ è½½æ•°æ®
    if is_main_process:
        logger.info(f"Loading data from {DATA_DIR}...")
    
    dataset = load_from_disk(DATA_DIR)
    
    # 2. åŠ è½½ Tokenizer
    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL_PATH)

    # 3. åˆå§‹åŒ–æ¨¡å‹
    if is_main_process:
        logger.info("Initializing BertLinear model...")
    
    model = BertLinear(
        bert_model_path=PRETRAINED_MODEL_PATH,
        num_labels=NUM_LABELS
    )

    # 4. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        
        # ä¼˜åŒ–å™¨ä¸è°ƒåº¦
        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_ratio=0.1,
        
        # è¯„ä¼°ä¸ä¿å­˜ç­–ç•¥
        eval_strategy=EVAL_STRATEGY,
        eval_steps=EVAL_STEPS,
        save_strategy=SAVE_STRATEGY,
        save_steps=SAVE_STEPS,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        
        # æ··åˆç²¾åº¦ä¸åŠ é€Ÿ
        fp16=False,
        bf16=True, # A40 æ”¯æŒ BF16
        dataloader_num_workers=8, # æé«˜æ•°æ®åŠ è½½é€Ÿåº¦
        
        # DDP é…ç½®
        ddp_find_unused_parameters=False, # å…³é”®ï¼šè®¾ä¸º False æé«˜é€Ÿåº¦ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ç§»é™¤äº† Pooler
        
        # æ—¥å¿—
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=50,
        report_to=REPORT_TO,
        
        # ç¦ç”¨ torch.compile (åŠ¨æ€ padding å¯¼è‡´æŒ‚èµ·)
        torch_compile=False 
    )

    # 5. Data Collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[
            EarlyStoppingCallback(early_stopping_patience=5),
            CustomLogCallback() # ğŸ†• æ·»åŠ è‡ªå®šä¹‰æ—¥å¿—å›è°ƒ
        ]
    )

    # 7. å¼€å§‹è®­ç»ƒ
    if is_main_process:
        logger.info("Starting training...")
    
    trainer.train()

    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if is_main_process:
        logger.info(f"Saving final model to {OUTPUT_DIR}/final_model")
        trainer.save_model(f"{OUTPUT_DIR}/final_model")
        # ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹æƒé‡
        torch.save(model.state_dict(), f"{OUTPUT_DIR}/final_model/pytorch_model.bin")

if __name__ == "__main__":
    main()
