import torch
import torch.nn as nn
from transformers import BertModel

class BertLinear(nn.Module):
    def __init__(self, bert_model_path, num_labels=14, dropout=0.1):
        super(BertLinear, self).__init__()
        
        # 1. åŠ è½½ BERT æ¨¡å‹
        # ğŸ†• add_pooling_layer=False: ä¸åŠ è½½ Pooler å±‚ï¼Œé¿å… DDP æŠ¥é”™ "unused parameters"
        self.bert = BertModel.from_pretrained(bert_model_path, add_pooling_layer=False)
        
        # 2. åŠ¨æ€è·å–åµŒå…¥ç»´åº¦
        embedding_dim = self.bert.config.hidden_size 
        print(f"âœ… BERT Embedding Dimension (Hidden Size) Detected: {embedding_dim}")

        # 3. åˆ†ç±»å±‚
        self.num_labels = num_labels
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(embedding_dim, num_labels)

    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """
        Activates gradient checkpointing for the underlying BERT model.
        """
        self.bert.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        # 1. BERT ç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        # ğŸ†• æ‰‹åŠ¨æå– [CLS] å‘é‡ (batch_size, hidden_size)
        # å› ä¸º add_pooling_layer=Falseï¼Œæ‰€ä»¥ outputs.pooler_output ä¸å¯ç”¨
        cls_token = outputs.last_hidden_state[:, 0, :]
        pooled_output = cls_token

        # 2. åˆ†ç±»
        pooled_output = self.dropout(pooled_output)
        logits = self.fc(pooled_output)
        
        # 3. è®¡ç®— Loss (å¦‚æœä¼ å…¥äº† labels)
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            return {"loss": loss, "logits": logits}
            
        return {"logits": logits}
