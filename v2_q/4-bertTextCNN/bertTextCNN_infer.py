import os
import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizerFast, DataCollatorWithPadding
from safetensors.torch import load_file
from bertTextCNN import BertTextCNN

# ================= é…ç½® =================
# è·¯å¾„é…ç½®
BASE_DIR = "/data/jinda/qinmingge/BERT/25fallnewsclassify"
TEST_FILE = os.path.join(BASE_DIR, "test_a.csv")
SUBMIT_FILE = os.path.join(BASE_DIR, "submit_cnn.csv")

# æ¨¡å‹è·¯å¾„
# 1. åŸå§‹é¢„è®­ç»ƒ BERT è·¯å¾„ (ç”¨äºåˆå§‹åŒ–æ¨¡å‹ç»“æ„)
PRETRAINED_BERT_PATH = os.path.join(BASE_DIR, "bert_small_4096_final")
# 2. å¾®è°ƒåçš„æƒé‡è·¯å¾„ (ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„å‚æ•°)
FT_MODEL_PATH = os.path.join(BASE_DIR, "output_ft_cnn/final_model/model.safetensors")

# è¶…å‚æ•°
# ğŸš€ å¤šå¡å¹¶è¡Œé…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,5" # æŒ‡å®šç©ºé—²æ˜¾å¡
BATCH_SIZE = 64 * 5 # 5å¼ å¡å¹¶è¡Œï¼ŒBatch Size ç¿» 5 å€
SEQ_LEN = 4096
NUM_LABELS = 14
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= æ•°æ®é›†å®šä¹‰ =================
class TestDataset(Dataset):
    def __init__(self, csv_file):
        self.df = pd.read_csv(csv_file, sep='\t' if csv_file.endswith('.tsv') else ',')
        # æ£€æŸ¥åˆ—åï¼Œå¦‚æœæ˜¯ 'text' åˆ™ä½¿ç”¨ï¼Œå¦åˆ™å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ–‡æœ¬
        if 'text' in self.df.columns:
            self.texts = self.df['text'].tolist()
        else:
            print(f"âš ï¸ Warning: 'text' column not found in {csv_file}. Using first column.")
            self.texts = self.df.iloc[:, 0].tolist()
            
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {"text": str(self.texts[idx])} # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²

# ================= ä¸»å‡½æ•° =================
def main():
    print(f"ğŸš€ Starting Inference on {DEVICE}...")
    
    # 1. åŠ è½½ Tokenizer
    print(f"Loading tokenizer from {PRETRAINED_BERT_PATH}...")
    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_BERT_PATH)
    
    # 2. å‡†å¤‡æ•°æ®
    print(f"Loading test data from {TEST_FILE}...")
    test_dataset = TestDataset(TEST_FILE)
    print(f"Test set size: {len(test_dataset)}")
    
    # è‡ªå®šä¹‰ collate_fn å¤„ç† Tokenization
    def collate_fn(batch):
        texts = [item["text"] for item in batch]
        # åŠ¨æ€ Padding åˆ°å½“å‰ Batch æœ€é•¿
        encoding = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=SEQ_LEN, 
            return_tensors="pt"
        )
        return encoding

    test_loader = DataLoader(
        test_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=16, # ğŸš€ å¢åŠ  CPU è¿›ç¨‹æ•°ä»¥åŒ¹é… 5 å¼  GPU çš„åå
        pin_memory=True
    )
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    print(f"Initializing model structure...")
    model = BertTextCNN(
        bert_model_path=PRETRAINED_BERT_PATH, 
        num_labels=NUM_LABELS,
        filter_sizes=(2, 3, 4, 5),
        num_filters=256 # âš ï¸ å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼bertTextCNN_FT.py ä¸­æ˜¯ 256
    )
    
    # 4. åŠ è½½å¾®è°ƒåçš„æƒé‡
    print(f"Loading weights from {FT_MODEL_PATH}...")
    if os.path.exists(FT_MODEL_PATH):
        state_dict = load_file(FT_MODEL_PATH)
        
        # å¤„ç†å¯èƒ½çš„ key ä¸åŒ¹é…é—®é¢˜ (ä¾‹å¦‚ DDP è®­ç»ƒå¯èƒ½å¯¼è‡´ module. å‰ç¼€)
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
                
        model.load_state_dict(new_state_dict)
        print("âœ… Weights loaded successfully!")
    else:
        raise FileNotFoundError(f"Model weights not found at {FT_MODEL_PATH}")
    
    # ğŸš€ å¯ç”¨ DataParallel å¤šå¡å¹¶è¡Œ
    if torch.cuda.device_count() > 1:
        print(f"ğŸ”¥ Using {torch.cuda.device_count()} GPUs for inference!")
        model = torch.nn.DataParallel(model)
    
    model.to(DEVICE)
    model.eval()
    
    # 5. æ¨ç†
    print("ğŸ”¥ Running inference...")
    all_preds = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Inferencing"):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            
            # BertTextCNN forward è¿”å› dict: {"logits": ...}
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs["logits"]
            
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            
    # 6. ä¿å­˜ç»“æœ
    print(f"Saving results to {SUBMIT_FILE}...")
    submit_df = pd.DataFrame({"label": all_preds})
    submit_df.to_csv(SUBMIT_FILE, index=False)
    
    print("ğŸ‰ Done! Check submit_result.csv")

if __name__ == "__main__":
    main()
