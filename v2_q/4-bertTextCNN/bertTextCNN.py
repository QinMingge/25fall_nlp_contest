import torch
import torch.nn as nn
from transformers import BertModel
import torch.nn.functional as F

class BertTextCNN(nn.Module):
    # bert_model_path åº”è¯¥æŒ‡å‘æ‚¨é¢„è®­ç»ƒä¿å­˜çš„ç›®å½•ï¼Œ
    def __init__(self, bert_model_path, num_labels=14, filter_sizes=(2, 3, 4, 5), num_filters=100):
        super(BertTextCNN, self).__init__()
        
        # 1. åŠ è½½ BERT æ¨¡åž‹ (ä¼šåŠ è½½æ‚¨é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡)
        # ðŸ†• add_pooling_layer=False: ä¸åŠ è½½ Pooler å±‚ï¼Œé¿å… DDP æŠ¥é”™ "unused parameters"
        self.bert = BertModel.from_pretrained(bert_model_path, add_pooling_layer=False)
        
        # 2. åŠ¨æ€èŽ·å–åµŒå…¥ç»´åº¦ D ï¼ˆD=256 æˆ– D=512ï¼‰
        embedding_dim = self.bert.config.hidden_size 
        print(f"âœ… BERT Embedding Dimension (Hidden Size) Detected: {embedding_dim}")
        
        # 3. TextCNN å·ç§¯å±‚å®šä¹‰ï¼šä½¿ç”¨ Conv1d æ›¿ä»£ Conv2dï¼Œæ•ˆçŽ‡æ›´é«˜
        # Conv1d è¾“å…¥: (Batch, Hidden, Seq)
        # Conv1d è¾“å‡º: (Batch, Out_Channels, Seq_Out)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embedding_dim, 
                      out_channels=num_filters, 
                      kernel_size=k) 
            for k in filter_sizes
        ])
        
        # 4. åˆ†ç±»å±‚
        self.num_labels = num_labels
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(num_filters * len(filter_sizes), num_labels)

    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """
        Activates gradient checkpointing for the underlying BERT model.
        """
        self.bert.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        # 1. BERT ç¼–ç 
        # æ³¨æ„ï¼šå¿…é¡»ä¼ å…¥ attention_maskï¼Œå¦åˆ™ BERT æ— æ³•åŒºåˆ† PAD
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        # last_hidden_state: (batch_size, seq_len, hidden_size)
        bert_output = outputs.last_hidden_state
        
        # 2. TextCNN å¤„ç†
        # Conv1d éœ€è¦è¾“å…¥ (batch_size, in_channels, seq_len)
        # bert_output æ˜¯ (batch_size, seq_len, hidden_size) -> permute -> (batch_size, hidden_size, seq_len)
        bert_output_cnn = bert_output.permute(0, 2, 1)

        conv_outputs = []
        for conv in self.convs:
            # å·ç§¯: (batch_size, num_filters, seq_len-k+1)
            conv_out = F.relu(conv(bert_output_cnn))
            
            # æ± åŒ–: Max-over-time pooling
            # (batch_size, num_filters, seq_len-k+1) -> max_pool -> (batch_size, num_filters, 1) -> squeeze -> (batch_size, num_filters)
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)

        # æ‹¼æŽ¥: (batch_size, num_filters * len(filter_sizes))
        concat_output = torch.cat(conv_outputs, 1)
        
        # 3. åˆ†ç±»
        dropout_output = self.dropout(concat_output)
        logits = self.fc(dropout_output)
        
        # 4. è®¡ç®— Loss (å¦‚æžœä¼ å…¥äº† labels)
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            return {"loss": loss, "logits": logits}
            
        return {"logits": logits}