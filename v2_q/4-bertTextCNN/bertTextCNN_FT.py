import os
import sys
import logging
import torch
import numpy as np
from datasets import load_from_disk
from transformers import (
    Trainer, 
    TrainingArguments, 
    BertTokenizerFast,
    EarlyStoppingCallback,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, f1_score
from bertTextCNN import BertTextCNN

# ========================== ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒº ==========================
# 1. è°ƒè¯•æ¨¡å¼å¼€å…³
DEBUG_MODE = False 

# 2. æ˜¾å¡æŒ‡å®š
if DEBUG_MODE:
    os.environ["CUDA_VISIBLE_DEVICES"] = "1"
else:
    # âš ï¸ è¯·æ ¹æ®å®é™…ç©ºé—²æ˜¾å¡ä¿®æ”¹æ­¤å¤„
    # ä¾‹å¦‚ï¼šå¦‚æœ 1,2,3 å·å¡ç©ºé—²ï¼Œåˆ™å¡« "1,2,3"
    # å¹¶åœ¨è¿è¡Œ torchrun æ—¶æŒ‡å®š --nproc_per_node=3
    os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,3,5" 
    # pass # å»ºè®®åœ¨å‘½ä»¤è¡Œé€šè¿‡ CUDA_VISIBLE_DEVICES æ§åˆ¶ï¼Œä¸è¦åœ¨ä»£ç é‡Œå†™æ­»ï¼Œé˜²æ­¢å†²çª

# 3. è·¯å¾„é…ç½®
DATA_DIR = "ft_data_stratified" # ğŸ†• æ›´æ–°ä¸ºæ–°çš„æ•°æ®ç›®å½•
PRETRAINED_MODEL_PATH = "bert_small_4096_final"

if DEBUG_MODE:
    OUTPUT_DIR = "output_ft_cnn_debug"
else:
    OUTPUT_DIR = "output_ft_cnn"

# 4. è®­ç»ƒè¶…å‚
SEQ_LEN = 4096
NUM_LABELS = 14

if DEBUG_MODE:
    NUM_EPOCHS = 1
    BATCH_SIZE = 4
    REPORT_TO = "none"
    SAVE_STRATEGY = "steps"
    EVAL_STRATEGY = "steps"
    EVAL_STEPS = 10
    SAVE_STEPS = 10
else:
    NUM_EPOCHS = 10
    # ğŸš€ æ¿€è¿›ä¼˜åŒ–ï¼šBatch Size 16 -> 32
    # æ˜¾å­˜åªç”¨äº†ä¸€åŠ (20GB/46GB)ï¼Œç›´æ¥ç¿»å€å¡«æ»¡æ˜¾å¡ï¼
    BATCH_SIZE = 32 
    REPORT_TO = "none"
    
    SAVE_STRATEGY = "steps"
    EVAL_STRATEGY = "steps"
    EVAL_STEPS = 500
    SAVE_STEPS = 500

# ====================================================================

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='macro')
    
    return {
        'accuracy': acc,
        'f1_macro': f1
    }

def main():
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    is_main_process = local_rank in [-1, 0]

    # 0. è®¾ç½®æ—¥å¿—
    # åªæœ‰ä¸»è¿›ç¨‹å†™æ—¥å¿—æ–‡ä»¶ï¼Œé¿å…å¤šè¿›ç¨‹å†™å…¥å†²çª
    if is_main_process:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        log_file = os.path.join(OUTPUT_DIR, "training_detailed.log")
        
        logging.basicConfig(
            format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
            datefmt="%m/%d/%Y %H:%M:%S",
            level=logging.INFO,
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(log_file, mode="w")
            ]
        )
        logger = logging.getLogger(__name__)
        logger.info(f"Logging to {log_file}")
    else:
        # å…¶ä»–è¿›ç¨‹åªè¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼Œä¸å†™æ–‡ä»¶
        logging.basicConfig(
            format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
            datefmt="%m/%d/%Y %H:%M:%S",
            level=logging.WARN
        )

    if is_main_process:
        print(f"\n{'='*40}")
        print(f"ğŸš€ Starting Fine-tuning (Stratified Split)...")
        print(f"ğŸ› ï¸  Mode: {'[DEBUG]' if DEBUG_MODE else '[FULL TRAINING]'}")
        print(f"{'='*40}\n")
    
    # 1. åŠ è½½æ•°æ®
    if not os.path.exists(DATA_DIR):
        raise FileNotFoundError(f"Data directory {DATA_DIR} not found. Run FT_data.py first.")
        
    if is_main_process:
        print(f"Loading data from {DATA_DIR}...")
    
    dataset = load_from_disk(DATA_DIR)
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
    
    if DEBUG_MODE:
        if is_main_process:
            print("âš ï¸ Debug mode: using small subset...")
        train_dataset = train_dataset.select(range(100))
        eval_dataset = eval_dataset.select(range(100))

    if is_main_process:
        print(f"Train size: {len(train_dataset)}")
        print(f"Eval size: {len(eval_dataset)}")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    if is_main_process:
        print(f"Initializing BertTextCNN from {PRETRAINED_MODEL_PATH}...")
    
    model = BertTextCNN(
        bert_model_path=PRETRAINED_MODEL_PATH, 
        num_labels=NUM_LABELS,
        filter_sizes=(2, 3, 4, 5),
        num_filters=256 
    )
    
    # 3. è®­ç»ƒå‚æ•°
    # å¼ºåˆ¶ç¦ç”¨ torch.compile
    os.environ["TORCH_COMPILE_DISABLE"] = "1"
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
    torch._dynamo.config.disable = True
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        
        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_ratio=0.1,
        
        eval_strategy=EVAL_STRATEGY, 
        eval_steps=EVAL_STEPS,
        save_strategy=SAVE_STRATEGY,
        save_steps=SAVE_STEPS,
        save_total_limit=2,
        
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        greater_is_better=True,
        
        bf16=True, 
        # torch_compile=False, # ğŸ”„ å½»åº•ç§»é™¤è¯¥å‚æ•°ï¼Œé˜²æ­¢ transformers è¯¯åˆ¤
        gradient_checkpointing=False, 
        dataloader_num_workers=8, # ğŸš€ å¢åŠ åˆ° 8ï¼šåˆ©ç”¨ 96 æ ¸ CPU åŠ é€Ÿæ•°æ®åŠ è½½
        dataloader_pin_memory=True, # ğŸ”„ æ¢å¤ä¸º Trueï¼šåŠ é€Ÿ CPU åˆ° GPU çš„æ•°æ®ä¼ è¾“
        
        logging_steps=50,
        report_to=REPORT_TO,
        
        ddp_find_unused_parameters=False # ğŸ”„ å…³é—­ï¼šæˆ‘ä»¬å·²ç»ä¿®æ”¹äº†æ¨¡å‹ï¼Œä¸å†æœ‰ unused parameters
    )

    # 4. Data Collator (åŠ¨æ€å¡«å……)
    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL_PATH)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")

    # 5. Trainer
    # å†æ¬¡ç¡®ä¿ model æ²¡æœ‰è¢«ç¼–è¯‘
    if hasattr(model, "_orig_mod"):
        model = model._orig_mod
        
    print(f"ğŸ” TrainingArguments.torch_compile = {training_args.torch_compile}")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
        data_collator=data_collator
    )

    # 6. å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("ğŸ”¥ Starting Training...")
    trainer.train()
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = f"{OUTPUT_DIR}/final_model"
    trainer.save_model(final_path)
    if is_main_process:
        print(f"âœ… Training done! Model saved to {final_path}")

if __name__ == "__main__":
    main()
