import os
import torch
from datasets import load_dataset
from transformers import (
    BertConfig, 
    BertForMaskedLM, 
    BertTokenizerFast, 
    DataCollatorForLanguageModeling, 
    Trainer, 
    TrainingArguments
)

# ========================== ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒº ==========================
# 1. è°ƒè¯•æ¨¡å¼å¼€å…³ (True = è·‘é€šæµç¨‹ / False = å…¨åŠ›å¼€ç«)
DEBUG_MODE = False  # <--- âš ï¸ è·‘æ­£å¼è®­ç»ƒå‰æ”¹æˆ False

# 2. æ˜¾å¡æŒ‡å®š (åªä½¿ç”¨ç©ºé—²çš„ 0,1,2,3)
# è¿™ä¸€è¡Œä¼šè®©ç¨‹åºåªçœ‹å¾—åˆ°è¿™4å¼ å¡ï¼Œç¼–å·ä¼šè‡ªåŠ¨é‡æ’ä¸º 0-3
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,4,6,7"

# 3. æ¨¡å‹ä¸æ•°æ®é…ç½®
MODEL_SIZE = 'small'   # 'mini' æˆ– 'small'
SEQ_LEN = 4096        # å¿…é¡»ä¸ pretrain_data.csv çš„åˆ‡åˆ†é•¿åº¦ä¸€è‡´
VOCAB_SIZE = 10000

# 4. è®­ç»ƒè¶…å‚ (æ ¹æ®æ˜¾å¡è‡ªåŠ¨è°ƒæ•´)
if DEBUG_MODE:
    NUM_EPOCHS = 1             # è°ƒè¯•åªè·‘ 1 è½®
    MAX_STEPS = 50             # æˆ–è€…åªè·‘ 50 æ­¥
    BATCH_SIZE = 8             # å°ä¸€ç‚¹é˜²æŠ¥é”™
    SAVE_STRATEGY = "no"       # è°ƒè¯•ä¸ä¿å­˜ä¸­é—´ç»“æœ
    REPORT_TO = "none"         # ä¸ä¸Šä¼  wandb
else:
    NUM_EPOCHS = 20 if MODEL_SIZE == 'mini' else 40
    MAX_STEPS = -1             # è·‘å®Œæ‰€æœ‰ Epoch
    # A40 æ˜¾å­˜å¾ˆå¤§ï¼ŒMini å¯ä»¥å¼€åˆ° 32ï¼ŒSmall å¯ä»¥å¼€åˆ° 12-16
    BATCH_SIZE = 32 if MODEL_SIZE == 'mini' else 16
    SAVE_STRATEGY = "epoch"    # æ¯è½®ä¿å­˜
    REPORT_TO = "none"         # å¦‚æœæœ‰ wandbè´¦å·å¯æ”¹æˆ "wandb"

# ====================================================================

def get_model_config():
    common_config = {
        "vocab_size": VOCAB_SIZE,
        "max_position_embeddings": SEQ_LEN,
        "type_vocab_size": 2,
        "hidden_dropout_prob": 0.1,
        "attention_probs_dropout_prob": 0.1,
    }
    
    if MODEL_SIZE == 'mini':
        return BertConfig(
            hidden_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            intermediate_size=1024,
            **common_config
        )
    elif MODEL_SIZE == 'small':
        return BertConfig(
            hidden_size=512,
            num_hidden_layers=8,
            num_attention_heads=8,
            intermediate_size=2048,
            **common_config
        )
    else:
        raise ValueError("Unknown MODEL_SIZE")

def main():
    # 1. æ£€æŸ¥ç¯å¢ƒ
    n_gpu = torch.cuda.device_count()
    print(f"ğŸš€ Detected {n_gpu} GPUs available for training.")
    print(f"ğŸ› ï¸  Mode: {'[DEBUG]' if DEBUG_MODE else '[FULL TRAINING]'}")
    
    # 2. Tokenizer
    tokenizer = BertTokenizerFast(vocab_file="vocab.txt", do_lower_case=False)
    
    # 3. åŠ è½½æ•°æ®
    data_file = "pretrain_data.csv"
    if DEBUG_MODE:
        print("âš ï¸ Loading only first 1000 lines for debugging...")
        # split='train[:1000]' è¿™æ˜¯ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åˆ‡ç‰‡å†™æ³•
        dataset = load_dataset("csv", data_files=data_file, split="train[:1000]")
    else:
        print(f"Loading full data from {data_file}...")
        dataset = load_dataset("csv", data_files=data_file, split="train")
    
    # 4. Tokenization
    def encode(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            max_length=SEQ_LEN, 
            padding="max_length"
        )

    # è°ƒè¯•æ¨¡å¼ç”¨å•è¿›ç¨‹ï¼Œæ­£å¼æ¨¡å¼ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    num_proc = 1 if DEBUG_MODE else 16
    print(f"Tokenizing data (num_proc={num_proc})...")
    tokenized_dataset = dataset.map(encode, batched=True, num_proc=num_proc, remove_columns=["text"])

    # 5. æ¨¡å‹åˆå§‹åŒ–
    config = get_model_config()
    model = BertForMaskedLM(config)
    
    # 6. è®­ç»ƒå‚æ•°
    output_dir = f"./output_{MODEL_SIZE}_{SEQ_LEN}_debug" if DEBUG_MODE else f"./output_{MODEL_SIZE}_{SEQ_LEN}"
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        max_steps=MAX_STEPS, # è°ƒè¯•æ—¶ç”Ÿæ•ˆ
        per_device_train_batch_size=BATCH_SIZE,
        
        # A40 ä¸“å±åŠ é€Ÿ
        bf16=True, 
        gradient_checkpointing=False, # æ˜¾å­˜å¤Ÿå¤§å…ˆä¸å¼€ï¼Œå¼€äº†çœæ˜¾å­˜ä½†æ…¢
        
        # DDP é…ç½®
        ddp_find_unused_parameters=False,
        
        # æ—¥å¿—ä¸ä¿å­˜
        save_strategy=SAVE_STRATEGY,
        logging_steps=10,
        learning_rate=1e-4,
        weight_decay=0.01,
        warmup_ratio=0.05,
        dataloader_num_workers=4,
        report_to=REPORT_TO
    )

    # 7. Trainer
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, 
        mlm=True, 
        mlm_probability=0.15
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset,
    )

    # 8. Run
    print("ğŸ”¥ Starting Training...")
    trainer.train()
    
    # 9. Save (ä»…æ­£å¼æ¨¡å¼æˆ–è°ƒè¯•è·‘å®Œå)
    final_path = f"./bert_{MODEL_SIZE}_{SEQ_LEN}_final"
    trainer.save_model(final_path)
    tokenizer.save_pretrained(final_path)
    print(f"âœ… All Done! Model saved to {final_path}")

if __name__ == "__main__":
    main()